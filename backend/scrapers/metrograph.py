import asyncio
import logging
import json
import os
import re
import random
import time
import traceback
import psutil
import functools
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
import sys
from logging.handlers import RotatingFileHandler

from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page

# è®¾ç½®æ—¥å¿—
def setup_logger(log_file=None):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    logger = logging.getLogger("metrograph_scraper")
    logger.setLevel(logging.INFO)
    logger.handlers = []  # æ¸…é™¤ç°æœ‰handlers

    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœæŒ‡å®šäº†æ—¥å¿—æ–‡ä»¶ï¼‰
    if log_file:
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        # ä½¿ç”¨RotatingFileHandleræ›¿ä»£FileHandlerï¼Œè®¾ç½®æœ€å¤§æ–‡ä»¶å¤§å°ä¸º10MBï¼Œä¿ç•™5ä¸ªå¤‡ä»½æ–‡ä»¶
        file_handler = RotatingFileHandler(
            log_file, 
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5,
            encoding='utf-8'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# å¼‚æ­¥é‡è¯•è£…é¥°å™¨
def async_retry(max_retries=3, retry_delay=2, backoff_factor=2):
    """å¼‚æ­¥é‡è¯•è£…é¥°å™¨ï¼Œæ”¯æŒæŒ‡æ•°é€€é¿"""
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            retries = 0
            while retries < max_retries:
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    retries += 1
                    if retries >= max_retries:
                        logger.error(f"{EMOJI['error']} å‡½æ•° {func.__name__} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œæ”¾å¼ƒ")
                        logger.error(f"æœ€åä¸€æ¬¡é”™è¯¯: {str(e)}")
                        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                        raise
                    
                    wait_time = retry_delay * (backoff_factor ** (retries - 1))
                    logger.warning(f"{EMOJI['warning']} å‡½æ•° {func.__name__} å‡ºé”™ (å°è¯• {retries}/{max_retries}): {str(e)}")
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
        return wrapper
    return decorator

# æ€§èƒ½è®¡æ—¶è£…é¥°å™¨
def log_execution_time(func):
    """è®°å½•å‡½æ•°æ‰§è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨çš„è£…é¥°å™¨"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        # è®°å½•å¼€å§‹å†…å­˜ä½¿ç”¨æƒ…å†µ
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        start_time = time.time()
        
        try:
            result = await func(*args, **kwargs)
            # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_diff = end_memory - start_memory
            
            logger.info(f"{EMOJI['perf']} å‡½æ•° {func.__name__} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’ï¼Œ"
                       f"å†…å­˜å˜åŒ–: {memory_diff:.2f} MB")
            return result
        except Exception as e:
            # è®°å½•é”™è¯¯æƒ…å†µä¸‹çš„æ—¶é—´å’Œå†…å­˜
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024
            memory_diff = end_memory - start_memory
            
            logger.error(f"{EMOJI['error']} å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥ï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’ï¼Œ"
                        f"å†…å­˜å˜åŒ–: {memory_diff:.2f} MB, é”™è¯¯: {str(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            raise
    return wrapper

# è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"metrograph_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logger(log_file)

# æ—¥å†é¡µé¢URL
CALENDAR_URL = "https://metrograph.com/calendar/"
HOME_URL = "https://metrograph.com/"
OUTPUT_DIR = "database"
OUTPUT_FILE = "metrograph_movies.json"

# ç‰¹æ®Šå­—ç¬¦è¡¨æƒ…
EMOJI = {
    "start": "âœ¨",
    "calendar": "ğŸ“…",
    "movie": "ğŸ¬",
    "success": "âœ…",
    "error": "âŒ",
    "warning": "âš ï¸",
    "info": "â„¹ï¸",
    "date": "ğŸ“†",
    "time": "â°",
    "loading": "â³",
    "perf": "âš¡",
    "debug": "ğŸ”",
    "memory": "ğŸ“Š"
}

# é€šç”¨å¼‚æ­¥é¡µé¢è¯·æ±‚å‡½æ•°
async def fetch_page_content(browser, url, timeout=30000):
    """é€šç”¨é¡µé¢å†…å®¹è·å–å‡½æ•°ï¼Œå¤„ç†åŠ è½½è¶…æ—¶å’Œé”™è¯¯"""
    logger.info(f"æ­£åœ¨è®¿é—®é¡µé¢: {url}")
    page = await browser.new_page()
    
    try:
        start_time = time.time()
        await page.goto(url, timeout=timeout)
        await page.wait_for_load_state("networkidle")
        load_time = time.time() - start_time
        logger.info(f"{EMOJI['perf']} é¡µé¢åŠ è½½è€—æ—¶: {load_time:.2f} ç§’")
        
        html_content = await page.content()
        return page, html_content
    except Exception as e:
        logger.error(f"{EMOJI['error']} è®¿é—®é¡µé¢ {url} æ—¶å‡ºé”™: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        await page.close()
        raise

@log_execution_time
async def scrape_metrograph_all():
    """æ•´åˆä¸»é¡µå’Œæ—¥å†é¡µé¢çš„ç”µå½±ä¿¡æ¯"""
    logger.info(f"{EMOJI['start']} å¼€å§‹çˆ¬å–Metrographç”µå½±ä¿¡æ¯...")
    
    # æ¸…ç†æ—§æ—¥å¿—
    cleanup_old_logs(log_dir)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", OUTPUT_DIR)
    os.makedirs(output_path, exist_ok=True)
    
    # ä½¿ç”¨å¼‚æ­¥çš„Playwright
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        
        try:
            # é¦–å…ˆä»æ—¥å†é¡µé¢è·å–ç”µå½±ä¿¡æ¯
            logger.info("ä»æ—¥å†é¡µé¢è·å–ç”µå½±ä¿¡æ¯...")
            calendar_movies = await scrape_metrograph_calendar(CALENDAR_URL, browser=browser)
            
            # ç„¶åä»ä¸»é¡µè·å–ç”µå½±ä¿¡æ¯
            logger.info("ä»ä¸»é¡µè·å–ç”µå½±ä¿¡æ¯...")
            homepage_movies = await scrape_metrograph_homepage(HOME_URL, browser=browser)
            
            # æ•´åˆä¸¤ä¸ªæ¥æºçš„ç”µå½±ä¿¡æ¯
            all_movies = merge_movie_data(calendar_movies, homepage_movies)
            
            # å°è¯•è¡¥å……ç”µå½±è¯¦æƒ…
            enriched_movies = await enrich_movie_details(all_movies, browser)
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            output_file = os.path.join(output_path, OUTPUT_FILE)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enriched_movies, f, ensure_ascii=False, indent=4)
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_movies = len(enriched_movies)
            movies_with_showtimes = sum(1 for movie in enriched_movies if movie["show_dates"])
            movies_without_showtimes = total_movies - movies_with_showtimes
            
            total_showtimes = 0
            total_dates = set()  # ä½¿ç”¨é›†åˆé¿å…é‡å¤è®¡æ•°
            
            for movie in enriched_movies:
                for date in movie["show_dates"]:
                    date_str = date["date"]
                    total_dates.add(date_str)
                    total_showtimes += len(date["times"])
            
            logger.info(f"{EMOJI['success']} çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²å­˜å…¥ {os.path.abspath(output_file)}")
            logger.info(f"  - æ€»è®¡: {total_movies} éƒ¨ç”µå½±")
            logger.info(f"  - æœ‰æ”¾æ˜ åœºæ¬¡: {movies_with_showtimes} éƒ¨")
            logger.info(f"  - æ— æ”¾æ˜ åœºæ¬¡: {movies_without_showtimes} éƒ¨")
            logger.info(f"  - æ€»æ”¾æ˜ æ—¥æœŸ: {len(total_dates)} ä¸ª")
            logger.info(f"  - æ€»æ”¾æ˜ åœºæ¬¡: {total_showtimes} åœº")
            
            # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
            process = psutil.Process(os.getpid())
            memory_usage = process.memory_info().rss / 1024 / 1024  # MB
            logger.info(f"{EMOJI['memory']} å½“å‰å†…å­˜ä½¿ç”¨: {memory_usage:.2f} MB")
            
            return enriched_movies
            
        finally:
            await browser.close()

def merge_movie_data(calendar_movies: List[Dict], homepage_movies: List[Dict]) -> List[Dict]:
    """
    æ•´åˆæ—¥å†é¡µé¢å’Œä¸»é¡µçš„ç”µå½±ä¿¡æ¯
    
    Args:
        calendar_movies: ä»æ—¥å†é¡µé¢è·å–çš„ç”µå½±åˆ—è¡¨
        homepage_movies: ä»ä¸»é¡µè·å–çš„ç”µå½±åˆ—è¡¨
    
    Returns:
        æ•´åˆåçš„ç”µå½±åˆ—è¡¨
    """
    # ä½¿ç”¨æ ‡é¢˜ä½œä¸ºé”®ï¼Œåˆ›å»ºå­—å…¸æ–¹ä¾¿æŸ¥æ‰¾å’Œåˆå¹¶
    movie_dict = {}
    
    # å…ˆåŠ å…¥æ—¥å†é¡µé¢çš„ç”µå½±ä¿¡æ¯
    for movie in calendar_movies:
        title = movie["title_en"]
        movie_dict[title] = movie
    
    # æ•´åˆä¸»é¡µç”µå½±çš„é¢å¤–ä¿¡æ¯
    for movie in homepage_movies:
        title = movie["title_en"]
        if title in movie_dict:
            # åˆå¹¶ç”µå½±ä¿¡æ¯ï¼Œä¿ç•™æ—¥å†é¡µé¢çš„æ”¾æ˜ ä¿¡æ¯
            if movie.get("director") and not movie_dict[title].get("director"):
                movie_dict[title]["director"] = movie["director"]
            if movie.get("year") and not movie_dict[title].get("year"):
                movie_dict[title]["year"] = movie["year"]
            if movie.get("image_url") and not movie_dict[title].get("image_url"):
                movie_dict[title]["image_url"] = movie["image_url"]
            if movie.get("duration") and not movie_dict[title].get("duration"):
                movie_dict[title]["duration"] = movie["duration"]
            if movie.get("language") and not movie_dict[title].get("language"):
                movie_dict[title]["language"] = movie["language"]
            if movie.get("overview_en") and not movie_dict[title].get("overview_en"):
                movie_dict[title]["overview_en"] = movie["overview_en"]
            if movie.get("trailer_url") and not movie_dict[title].get("trailer_url"):
                movie_dict[title]["trailer_url"] = movie["trailer_url"]
            if movie.get("note") and not movie_dict[title].get("note"):
                movie_dict[title]["note"] = movie["note"]
        else:
            # å¦‚æœæ˜¯ä¸»é¡µç‹¬æœ‰çš„ç”µå½±ï¼Œä¹ŸåŠ å…¥æ•´åˆåçš„åˆ—è¡¨
            movie_dict[title] = movie
    
    # å°†å­—å…¸è½¬æ¢å›åˆ—è¡¨
    return list(movie_dict.values())

async def scrape_metrograph_homepage(url: str, browser=None) -> List[Dict]:
    """
    çˆ¬å–Metrographä¸»é¡µçš„ç”µå½±ä¿¡æ¯
    
    Args:
        url: ä¸»é¡µURL
        browser: æµè§ˆå™¨å®ä¾‹
    
    Returns:
        ç”µå½±ä¿¡æ¯åˆ—è¡¨
    """
    logger.info(f"å¼€å§‹çˆ¬å–Metrographä¸»é¡µ: {url}")
    
    close_browser = False
    if browser is None:
        p = await async_playwright()
        browser = await p.chromium.launch()
        close_browser = True
    
    page = await browser.new_page()
    movies = []

    try:
        await page.goto(url, timeout=30000)
        await page.wait_for_load_state("networkidle")
        
        # è·å–é¡µé¢å†…å®¹
        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾ä¸»é¡µçš„ç”µå½±å¡ç‰‡/åŒºå—
        movie_cards = soup.select('div.film-card') or soup.select('.film-item') or soup.select('.now-playing-item')
        
        for card in movie_cards:
            movie = {}
            
            # æå–æ ‡é¢˜
            title_tag = card.select_one('h3.title') or card.select_one('.title') or card.select_one('h3')
            if title_tag:
                movie["title_en"] = title_tag.get_text(strip=True)
            else:
                continue  # å¦‚æœæ‰¾ä¸åˆ°æ ‡é¢˜ï¼Œè·³è¿‡æ­¤å¡ç‰‡
            
            # æå–è¯¦æƒ…é“¾æ¥
            detail_link = None
            link_tag = card.select_one('a')
            if link_tag:
                detail_link = link_tag.get('href', '')
                if detail_link and not detail_link.startswith('http'):
                    detail_link = f"https://metrograph.com{detail_link}"
                movie["detail_url"] = detail_link
            
            # æå–å›¾ç‰‡
            img_tag = card.select_one('img')
            if img_tag:
                img_url = img_tag.get('src', '')
                if img_url and not img_url.startswith('http'):
                    img_url = f"https://metrograph.com{img_url}"
                movie["image_url"] = img_url
            
            # æå–å…¶ä»–åŸºæœ¬ä¿¡æ¯
            info_tag = card.select_one('.info') or card.select_one('.meta')
            if info_tag:
                # æå–å¯¼æ¼”
                director_tag = info_tag.select_one('.director') or info_tag.find(string=re.compile(r'Director', re.IGNORECASE))
                if director_tag:
                    director_text = director_tag.get_text(strip=True) if hasattr(director_tag, 'get_text') else str(director_tag)
                    director_match = re.search(r'Director[s]?:\s*([^,]+)', director_text, re.IGNORECASE)
                    if director_match:
                        movie["director"] = director_match.group(1).strip()
                
                # æå–å¹´ä»½
                year_tag = info_tag.select_one('.year') or info_tag.find(string=re.compile(r'\b\d{4}\b'))
                if year_tag:
                    year_text = year_tag.get_text(strip=True) if hasattr(year_tag, 'get_text') else str(year_tag)
                    year_match = re.search(r'\b(\d{4})\b', year_text)
                    if year_match:
                        movie["year"] = int(year_match.group(1))
                
                # æå–æ—¶é•¿
                duration_tag = info_tag.select_one('.duration') or info_tag.find(string=re.compile(r'\d+\s*min'))
                if duration_tag:
                    duration_text = duration_tag.get_text(strip=True) if hasattr(duration_tag, 'get_text') else str(duration_tag)
                    duration_match = re.search(r'(\d+)\s*min', duration_text)
                    if duration_match:
                        movie["duration"] = f"{duration_match.group(1)} min"
            
            # æå–ç®€ä»‹
            desc_tag = card.select_one('.description') or card.select_one('.overview')
            if desc_tag:
                movie["overview_en"] = desc_tag.get_text(strip=True)
            
            # è®¾ç½®å½±é™¢ä¿¡æ¯
            movie["cinema"] = "Metrograph"
            
            # æ·»åŠ ç©ºçš„æ”¾æ˜ ä¿¡æ¯ï¼ˆå°†åœ¨åç»­åˆå¹¶ä¸­ä¿ç•™æ—¥å†æ•°æ®ï¼‰
            movie["show_dates"] = []
            
            movies.append(movie)
    
    except Exception as e:
        logger.error(f"çˆ¬å–ä¸»é¡µæ—¶å‡ºé”™: {str(e)}")
    
    finally:
        await page.close()
        if close_browser:
            await browser.close()
    
    logger.info(f"ä»ä¸»é¡µè·å–äº† {len(movies)} éƒ¨ç”µå½±çš„ä¿¡æ¯")
    return movies

@log_execution_time
async def scrape_metrograph_calendar(
    url: str = CALENDAR_URL,
    max_retries: int = 3,
    retry_delay: int = 2,
    browser = None
) -> List[Dict]:
    """
    çˆ¬å–Metrographæ—¥å†é¡µé¢ï¼Œå…ˆè§£ææ—¥å†è·å–å¯ç”¨æ—¥æœŸï¼Œç„¶åè®¿é—®æ¯ä¸ªæ—¥æœŸé¡µé¢çˆ¬å–ç”µå½±ä¿¡æ¯
    
    Args:
        url: æ—¥å†é¡µé¢URL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        browser: å·²æœ‰çš„æµè§ˆå™¨å®ä¾‹
    
    Returns:
        åŒ…å«æ‰€æœ‰ç”µå½±æ”¾æ˜ ä¿¡æ¯çš„åˆ—è¡¨
    """
    logger.info(f"{EMOJI['calendar']} å¼€å§‹çˆ¬å–Metrographæ—¥å†ä¿¡æ¯: {url}")
    
    close_browser = False
    if browser is None:
        p = await async_playwright()
        browser = await p.chromium.launch()
        close_browser = True
    
    try:
        # é¦–å…ˆè®¿é—®æ—¥å†é¡µé¢ï¼Œè·å–æ‰€æœ‰æœ‰æ•ˆæ—¥æœŸ
        available_dates = await get_available_dates(browser, url, max_retries, retry_delay)
        logger.info(f"ä»æ—¥å†ä¸­æ‰¾åˆ° {len(available_dates)} ä¸ªæœ‰æ•ˆæ”¾æ˜ æ—¥æœŸ")
        
        if not available_dates:
            logger.error(f"{EMOJI['error']} æœªèƒ½ä»æ—¥å†é¡µé¢è·å–æœ‰æ•ˆæ—¥æœŸ")
            if close_browser:
                await browser.close()
            return []
        
        # ç”¨äºå­˜å‚¨æ‰€æœ‰ç”µå½±æ•°æ®çš„å­—å…¸
        all_movies = {}
        
        # è®¿é—®æ¯ä¸ªæ—¥æœŸé¡µé¢
        for i, date_str in enumerate(available_dates):
            date_url = f"{url}?date={date_str}"
            logger.info(f"[{i+1}/{len(available_dates)}] çˆ¬å–æ—¥æœŸ {date_str} çš„ç”µå½±ä¿¡æ¯: {date_url}")
            
            # çˆ¬å–è¯¥æ—¥æœŸé¡µé¢çš„ç”µå½±ä¿¡æ¯
            date_movies = await scrape_calendar_page(date_url, max_retries, retry_delay, browser)
            logger.info(f"  - ä»è¯¥æ—¥æœŸé¡µé¢è·å–äº† {len(date_movies)} éƒ¨ç”µå½±çš„ä¿¡æ¯")
            
            # åˆå¹¶ç”µå½±æ•°æ®
            for movie in date_movies:
                title = movie["title_en"]
                if title in all_movies:
                    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ—¥æœŸçš„æ”¾æ˜ ä¿¡æ¯
                    for new_date in movie["show_dates"]:
                        date_exists = False
                        for existing_date in all_movies[title]["show_dates"]:
                            if existing_date["date"] == new_date["date"]:
                                # å¦‚æœå·²æœ‰ç›¸åŒæ—¥æœŸï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒæ—¶é—´çš„æ”¾æ˜ 
                                for new_time in new_date["times"]:
                                    time_exists = False
                                    for existing_time in existing_date["times"]:
                                        if existing_time["time"] == new_time["time"]:
                                            time_exists = True
                                            break
                                    if not time_exists:
                                        existing_date["times"].append(new_time)
                                date_exists = True
                                break
                        if not date_exists:
                            all_movies[title]["show_dates"].append(new_date)
                else:
                    all_movies[title] = movie
        
        if close_browser:
            await browser.close()
        
        # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨
        movies_list = list(all_movies.values())
        logger.info(f"{EMOJI['movie']} å…±è·å–åˆ° {len(movies_list)} éƒ¨ç”µå½±çš„æ”¾æ˜ ä¿¡æ¯")
        
        # æ ¼å¼åŒ–æ—¥æœŸï¼Œä½¿å…¶ä¸Film Forumæ ¼å¼ä¸€è‡´ (YYYY-MM-DD)
        logger.info(f"{EMOJI['date']} æ­£åœ¨æ ¼å¼åŒ–æ—¥æœŸä¸ºæ ‡å‡†æ ¼å¼ (YYYY-MM-DD)...")
        date_format_count = 0
        for movie in movies_list:
            for date_info in movie["show_dates"]:
                original_date = date_info["date"]
                try:
                    # å°è¯•è§£æå¦‚ "Friday March 28, 2025" æ ¼å¼çš„æ—¥æœŸ
                    parsed_date = datetime.strptime(original_date, "%A %B %d, %Y")
                    date_info["date"] = parsed_date.strftime("%Y-%m-%d")
                    date_format_count += 1
                except ValueError:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¿ç•™åŸå§‹æ ¼å¼
                    logger.warning(f"{EMOJI['warning']} æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {original_date}")
        logger.info(f"  - æˆåŠŸæ ¼å¼åŒ– {date_format_count} ä¸ªæ—¥æœŸ")
        
        return movies_list
    
    except Exception as e:
        logger.error(f"{EMOJI['error']} çˆ¬å–æ—¥å†é¡µé¢æ—¶å‡ºé”™: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        if close_browser:
            await browser.close()
        return []

@log_execution_time
async def get_available_dates(browser, url: str, max_retries: int, retry_delay: int) -> List[str]:
    """
    ä»æ—¥å†é¡µé¢è·å–æ‰€æœ‰æœ‰æ•ˆæ”¾æ˜ æ—¥æœŸ
    
    Args:
        browser: Playwrightæµè§ˆå™¨å®ä¾‹
        url: æ—¥å†é¡µé¢URL
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
    
    Returns:
        å¯ç”¨æ—¥æœŸåˆ—è¡¨ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD
    """
    page = await browser.new_page()
    available_dates = []
    retries = 0
    
    while retries < max_retries:
        try:
            logger.info(f"è®¿é—®æ—¥å†é¡µé¢è·å–å¯ç”¨æ—¥æœŸ: {url}")
            start_time = time.time()
            await page.goto(url)
            await page.wait_for_load_state("networkidle")
            load_time = time.time() - start_time
            logger.info(f"{EMOJI['perf']} é¡µé¢åŠ è½½è€—æ—¶: {load_time:.2f} ç§’")
            
            # è·å–é¡µé¢å†…å®¹
            html_content = await page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æŸ¥æ‰¾æ‰€æœ‰æ—¥å†æœˆä»½
            calendar_months = soup.select('div.calendar_month')
            
            for month in calendar_months:
                # è·å–æ¯ä¸ªæœˆä¸­çš„æ‰€æœ‰æ—¥æœŸæ¡ç›®
                date_items = month.select('li[data-thisdate]')
                
                for item in date_items:
                    # è·å–æ—¥æœŸå€¼
                    date_str = item.get('data-thisdate')
                    if not date_str:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¯ä»¥æŸ¥çœ‹æ”¾æ˜ ä¿¡æ¯çš„æ—¥æœŸï¼ˆépastå’Œunscheduledï¼‰
                    is_past = 'past' in item.get('class', [])
                    is_unscheduled = 'unscheduled' in item.get('class', [])
                    title = item.get('title', '')
                    
                    if not is_past and not is_unscheduled and ('See showtimes' in title or 'Today' in title):
                        available_dates.append(date_str)
                        logger.debug(f"{EMOJI['debug']} æ·»åŠ å¯ç”¨æ—¥æœŸ: {date_str}")
            
            break
        except Exception as e:
            retries += 1
            logger.error(f"è·å–æ—¥å†æ—¥æœŸæ—¶å‡ºé”™ (å°è¯• {retries}/{max_retries}): {str(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            if retries < max_retries:
                wait_time = retry_delay * (2 ** (retries - 1))
                logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                await asyncio.sleep(wait_time)
            else:
                logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒè·å–æ—¥å†æ—¥æœŸ")
    
    await page.close()
    return available_dates

async def scrape_calendar_page(
    url: str,
    max_retries: int = 3,
    retry_delay: int = 2,
    browser = None
) -> List[Dict]:
    """
    çˆ¬å–æŒ‡å®šæ—¥æœŸçš„Metrographç”µå½±æ—¥å†é¡µé¢å¹¶è§£æç”µå½±æ”¾æ˜ ä¿¡æ¯
    
    Args:
        url: æ—¥å†é¡µé¢URLï¼ŒåŒ…å«æ—¥æœŸå‚æ•°
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        browser: å·²æœ‰çš„æµè§ˆå™¨å®ä¾‹ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨æ­¤å®ä¾‹ï¼Œå¦åˆ™åˆ›å»ºæ–°å®ä¾‹
    
    Returns:
        åŒ…å«ç”µå½±æ”¾æ˜ ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    logger.info(f"å¼€å§‹ä»é¡µé¢çˆ¬å–ç”µå½±ä¿¡æ¯: {url}")
    
    html_content = None
    retries = 0
    close_browser = False
    
    # å¦‚æœæœªæä¾›æµè§ˆå™¨å®ä¾‹ï¼Œåˆ›å»ºæ–°å®ä¾‹
    if browser is None:
        p = await async_playwright()
        browser = await p.chromium.launch()
        close_browser = True
    
    page = await browser.new_page()
    
    try:
        while retries < max_retries:
            try:
                logger.info(f"è®¿é—®é¡µé¢: {url}")
                await page.goto(url)
                logger.info("é¡µé¢åŠ è½½å®Œæˆï¼Œç­‰å¾…ç½‘ç»œç©ºé—²...")
                await page.wait_for_load_state("networkidle")
                
                html_content = await page.content()
                break
            except Exception as e:
                retries += 1
                logger.error(f"åŠ è½½é¡µé¢æ—¶å‡ºé”™ (å°è¯• {retries}/{max_retries}): {str(e)}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                if retries < max_retries:
                    wait_time = retry_delay * (2 ** (retries - 1))  # æŒ‡æ•°é€€é¿
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒçˆ¬å–")
                    await page.close()
                    if close_browser:
                        await browser.close()
                    return []
        
        if not html_content:
            logger.error("æœªèƒ½è·å–é¡µé¢å†…å®¹")
            await page.close()
            if close_browser:
                await browser.close()
            return []
        
        logger.info("å¼€å§‹è§£æé¡µé¢å†…å®¹...")
        movies_data = parse_calendar_page(html_content)
        
        # å¦‚æœæ˜¯éœ€è¦ä¸°å¯Œç”µå½±è¯¦æƒ…çš„åœºæ™¯ï¼Œåˆ™è°ƒç”¨enrich_movie_details
        # enriched_movies = await enrich_movie_details(movies_data, browser)
        
        await page.close()
        if close_browser:
            await browser.close()
            
        return movies_data
    
    except Exception as e:
        logger.error(f"çˆ¬å–é¡µé¢æ—¶å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        await page.close()
        if close_browser:
            await browser.close()
        return []

def parse_calendar_page(html_content: str) -> List[Dict]:
    """
    è§£æHTMLå†…å®¹ï¼Œæå–ç”µå½±æ”¾æ˜ ä¿¡æ¯
    
    Args:
        html_content: HTMLå†…å®¹
    
    Returns:
        åŒ…å«ç”µå½±ä¿¡æ¯çš„åˆ—è¡¨
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # åˆå§‹åŒ–å¹´ä»½å˜é‡
        year = None
    
    # å°è¯•ä»é¡µé¢è·å–å½“å‰å¹´ä»½
    now_time_tag = soup.find(string=re.compile(r'Now time \d{4}-\d{2}-\d{2}'))
    if now_time_tag:
        year_match = re.search(r'(\d{4})-\d{2}-\d{2}', now_time_tag)
            if year_match:
            year = year_match.group(1)
            logger.info(f"ä»'Now time'æ ‡è®°ä¸­æå–åˆ°å¹´ä»½: {year}")
    
    if not year:
        # å¦‚æœæ‰¾ä¸åˆ°å¹´ä»½ï¼Œé»˜è®¤ä¸ºå½“å‰å¹´ä»½
        from datetime import datetime
        year = str(datetime.now().year)
        logger.info(f"æœªæ‰¾åˆ°å¹´ä»½æ ‡è®°ï¼Œä½¿ç”¨å½“å‰å¹´ä»½: {year}")
    
    # æ‰¾åˆ°æ‰€æœ‰æ—¥æœŸåŒºå— - ä¿®å¤é€‰æ‹©å™¨
    date_blocks = soup.select('div.calendar-list-day')
    if len(date_blocks) == 0:
        # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
        date_blocks = soup.select('.calendar-list-day')
        if len(date_blocks) == 0:
            # å†å°è¯•å…¶ä»–é€‰æ‹©å™¨
            date_blocks = soup.select('li.day-section')
            if len(date_blocks) == 0:
                # æœ€åå°è¯•
                date_blocks = soup.find_all('div', class_=lambda c: c and 'calendar-list-day' in c)
                
    logger.info(f"æ‰¾åˆ° {len(date_blocks)} ä¸ªæ—¥æœŸåŒºå—")
    
    # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ—¥æœŸåŒºå—ï¼Œè®°å½•HTMLç»“æ„ä»¥ä¾¿è°ƒè¯•
    if len(date_blocks) == 0:
        logger.warning("æ— æ³•æ‰¾åˆ°ä»»ä½•æ—¥æœŸåŒºå—ï¼Œè®°å½•HTMLç»“æ„ä»¥ä¾¿è°ƒè¯•")
        with open("calendar_html_debug.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        logger.warning("å·²å°†HTMLå†…å®¹ä¿å­˜åˆ°calendar_html_debug.htmlæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥HTMLç»“æ„")
        
        # å°è¯•æŸ¥æ‰¾é¡µé¢ä¸­çš„å…³é”®å…ƒç´ æ¥å¸®åŠ©è¯†åˆ«ç»“æ„
        logger.info("å°è¯•è¯†åˆ«é¡µé¢ç»“æ„:")
        for tag in soup.select('div[class]'):
            class_attr = tag.get('class', [])
            if class_attr and any('calendar' in c.lower() for c in class_attr):
                logger.info(f"å‘ç°å¯èƒ½ç›¸å…³çš„å…ƒç´ : {tag.name}.{' '.join(class_attr)}")
    
    # ç”¨äºä¿å­˜æ‰€æœ‰ç”µå½±æ•°æ®çš„å­—å…¸ï¼Œé”®ä¸ºç”µå½±æ ‡é¢˜ï¼Œå€¼ä¸ºç”µå½±ä¿¡æ¯
    # è¿™æ ·å¯ä»¥åˆå¹¶åŒä¸€ç”µå½±åœ¨ä¸åŒæ—¥æœŸçš„åœºæ¬¡
    movies_dict = {}
    
    # éå†æ¯ä¸ªæ—¥æœŸåŒºå—
    for date_block in date_blocks:
        # æå–æ—¥æœŸ - ä¿®å¤æ—¥æœŸé€‰æ‹©å™¨
        date_heading = date_block.select_one('h2.date-heading')
        if not date_heading:
            date_heading = date_block.select_one('.date')
            if not date_heading:
                date_heading = date_block.select_one('h2') or date_block.select_one('h3')
                if not date_heading:
                    continue

        date_text = date_heading.get_text(strip=True)
        logger.info(f"å¤„ç†æ—¥æœŸ: {date_text}")
        
        # ç¡®ä¿æ—¥æœŸåŒ…å«å¹´ä»½
        if year and year not in date_text:
            date_text = f"{date_text}, {year}"
        
        # æå–è¯¥æ—¥æœŸä¸‹çš„æ‰€æœ‰ç”µå½±æ¡ç›® - ä¿®å¤ç”µå½±æ¡ç›®é€‰æ‹©å™¨
        movie_entries = date_block.select('div.movie-entry')
        if len(movie_entries) == 0:
            movie_entries = date_block.select('.item') or date_block.select('div.item')
            if len(movie_entries) == 0:
                movie_entries = date_block.select('li') or date_block.select('.movie')
        
        logger.info(f"  åœ¨æ­¤æ—¥æœŸä¸‹æ‰¾åˆ° {len(movie_entries)} ä¸ªç”µå½±æ¡ç›®")
        
        # éå†æ¯ä¸ªç”µå½±æ¡ç›®
        for movie_entry in movie_entries:
            # æå–ç”µå½±æ ‡é¢˜ - ä¿®å¤æ ‡é¢˜é€‰æ‹©å™¨
            title_tag = movie_entry.select_one('h3.movie-entry-title a')
            if not title_tag:
                title_tag = movie_entry.select_one('a.title') or movie_entry.select_one('.title a') or movie_entry.select_one('a')
                if not title_tag:
                    continue

            title = title_tag.get_text(strip=True)
            detail_url = title_tag.get('href', '')
            if detail_url and not detail_url.startswith('http'):
                detail_url = f"https://metrograph.com{detail_url}"
            
            # æå–åœºæ¬¡ä¿¡æ¯ - ä¿®å¤æ”¾æ˜ æ—¶é—´é€‰æ‹©å™¨
            showtime_items = movie_entry.select('ul.movie-entry-showtimes li')
            if len(showtime_items) == 0:
                showtimes_container = movie_entry.select_one('.calendar-list-showtimes') or movie_entry.select_one('.showtimes')
                if showtimes_container:
                    time_links = showtimes_container.select('a')
                    # ç¡®ä¿è·³è¿‡ç¬¬ä¸€ä¸ªé“¾æ¥å¦‚æœå®ƒæ˜¯æ ‡é¢˜é“¾æ¥
                    start_idx = 1 if len(time_links) > 1 and time_links[0].get_text(strip=True) == title else 0
                    showtime_items = time_links[start_idx:]
            
            showtimes = []
            
            for item in showtime_items:
                time_text = item.get_text(strip=True)
                
                # æ£€æŸ¥æ˜¯å¦å”®ç½„ - å¢å¼ºæ£€æµ‹é€»è¾‘
                sold_out = False
                # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨ç¤ºå”®ç½„çš„çº¢è‰²æ–‡æœ¬
                if item.select_one('span.text-red'):
                    sold_out = True
                # æ£€æŸ¥classä¸­æ˜¯å¦æœ‰sold_outæˆ–sold-outæ ‡è®°
                item_classes = item.get('class', [])
                if item_classes and ('sold_out' in item_classes or 'sold-out' in item_classes):
                    sold_out = True
                # æ£€æŸ¥æ˜¯å¦æœ‰å”®ç½„çš„æ–‡æœ¬æç¤º
                if 'sold out' in time_text.lower() or 'sold-out' in time_text.lower():
                    sold_out = True
                # æ£€æŸ¥aæ ‡ç­¾æ˜¯å¦æœ‰sold_outç±»
                a_tag = item.select_one('a')
                if a_tag and a_tag.get('class') and ('sold_out' in a_tag.get('class') or 'sold-out' in a_tag.get('class')):
                    sold_out = True
                
                # æå–è´­ç¥¨é“¾æ¥
                ticket_link = None
                if hasattr(item, 'name') and item.name == 'a':
                    link_tag = item
                else:
                    link_tag = item.select_one('a')
                
                if link_tag and not sold_out:
                    ticket_link = link_tag.get('href', '')
                    if ticket_link and not ticket_link.startswith('http'):
                        if 'ticketing' in ticket_link.lower():
                            ticket_link = f"https://t.metrograph.com{ticket_link}"
                        else:
                            ticket_link = f"https://metrograph.com{ticket_link}"
                
                showtimes.append({
                        "time": time_text,
                    "ticket_url": ticket_link,
                        "sold_out": sold_out
                    })

            # å¦‚æœè¯¥ç”µå½±å·²å­˜åœ¨ï¼Œæ·»åŠ æ–°çš„æ”¾æ˜ æ—¥æœŸ
            if title in movies_dict:
                movies_dict[title]["show_dates"].append({
                    "date": date_text,
                    "times": showtimes
                })
            else:
                # å¦åˆ™åˆ›å»ºæ–°çš„ç”µå½±æ¡ç›®
                movies_dict[title] = {
                    "title_en": title,
                    "show_dates": [{
                        "date": date_text,
                        "times": showtimes
                    }],
                    "detail_url": detail_url,
                    "image_url": "",
                    "director": "",
                    "year": None,
                    "cinema": "Metrograph"
                }
            
            logger.info(f"  - ç”µå½± '{title}': {len(showtimes)} ä¸ªæ”¾æ˜ æ—¶é—´")
    
    # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨
    movies_list = list(movies_dict.values())
    
    # è®¡ç®—æ”¾æ˜ åœºæ¬¡æ€»æ•°
    total_showtimes = 0
    for movie in movies_list:
        for date in movie["show_dates"]:
            total_showtimes += len(date["times"])
    
    logger.info(f"ä»æ—¥å†é¡µé¢æå–äº† {len(movies_list)} éƒ¨ç”µå½±çš„æ”¾æ˜ ä¿¡æ¯ï¼Œå…± {total_showtimes} åœºæ”¾æ˜ ")
    
    return movies_list

async def enrich_movie_details(movies: List[Dict], browser) -> List[Dict]:
    """
    è®¿é—®ç”µå½±è¯¦æƒ…é¡µé¢ï¼Œè¡¥å……ç”µå½±ä¿¡æ¯
    
    Args:
        movies: ç”µå½±ä¿¡æ¯åˆ—è¡¨
        browser: å·²ç»åˆ›å»ºçš„æµè§ˆå™¨å®ä¾‹
    
    Returns:
        è¡¥å……äº†è¯¦ç»†ä¿¡æ¯çš„ç”µå½±åˆ—è¡¨
    """
    logger.info(f"{EMOJI['info']} å¼€å§‹enriching {len(movies)} éƒ¨ç”µå½±çš„è¯¦ç»†ä¿¡æ¯...")
    
    enriched_movies = []
    
    try:
        page = await browser.new_page()
        
        for i, movie in enumerate(movies):
            logger.info(f"[{i+1}/{len(movies)}] Enriching movie: {movie['title_en']}")
            
            # å¦‚æœæ²¡æœ‰è¯¦æƒ…é¡µURLï¼Œè·³è¿‡
            if not movie["detail_url"]:
                logger.info(f"  - æ²¡æœ‰è¯¦æƒ…é¡µURLï¼Œè·³è¿‡")
                enriched_movies.append(movie)
                continue
            
            try:
                # è®¿é—®è¯¦æƒ…é¡µ
                logger.info(f"  - æ­£åœ¨è®¿é—®è¯¦æƒ…é¡µ: {movie['detail_url']}")
                await page.goto(movie["detail_url"], timeout=30000)
                await page.wait_for_load_state("networkidle")
                
                # æå–è¯¦ç»†ä¿¡æ¯
                movie_details = await extract_movie_details(page)
                
                # è·å–è¯¦ç»†çš„æ”¾æ˜ ä¿¡æ¯ï¼ŒåŒ…æ‹¬Q&Aå’Œç‰¹åˆ«æ´»åŠ¨
                detailed_showtimes = await extract_showtimes_with_details(page)
                
                # æ‰“å°æŸ¥æ‰¾åˆ°çš„è¯¦ç»†ä¿¡æ¯
                detail_found = []
                
                # æ›´æ–°ç”µå½±ä¿¡æ¯
                if movie_details:
                    if movie_details.get("director") and not movie.get("director"):
                        movie["director"] = movie_details["director"]
                        detail_found.append(f"å¯¼æ¼”: {movie_details['director']}")
                    
                    if movie_details.get("year") and not movie.get("year"):
                        movie["year"] = movie_details["year"]
                        detail_found.append(f"å¹´ä»½: {movie_details['year']}")
                    
                    if movie_details.get("image_url") and not movie.get("image_url"):
                        movie["image_url"] = movie_details["image_url"]
                        detail_found.append("è·å–åˆ°æµ·æŠ¥å›¾ç‰‡")
                    
                    if movie_details.get("duration") and not movie.get("duration"):
                        movie["duration"] = movie_details["duration"]
                        detail_found.append(f"æ—¶é•¿: {movie_details['duration']}")
                    
                    if movie_details.get("language") and not movie.get("language"):
                        movie["language"] = movie_details["language"]
                        detail_found.append(f"è¯­è¨€: {movie_details['language']}")
                    
                    if movie_details.get("overview_en") and not movie.get("overview_en"):
                        movie["overview_en"] = movie_details["overview_en"]
                        overview_summary = movie_details["overview_en"][:50] + "..." if len(movie_details["overview_en"]) > 50 else movie_details["overview_en"]
                        detail_found.append(f"ç®€ä»‹: {overview_summary}")
                    
                    if movie_details.get("has_qa") is not None:
                        movie["has_qa"] = movie_details["has_qa"]
                        if movie["has_qa"] and movie_details.get("qa_details"):
                            movie["qa_details"] = movie_details["qa_details"]
                            detail_found.append(f"Q&Aä¿¡æ¯: {movie_details['qa_details'][:50]}..." if len(movie_details['qa_details']) > 50 else movie_details['qa_details'])
                    
                    if movie_details.get("has_introduction") is not None:
                        movie["has_introduction"] = movie_details["has_introduction"]
                        if movie["has_introduction"] and movie_details.get("introduction_details"):
                            movie["introduction_details"] = movie_details["introduction_details"]
                            detail_found.append(f"ä»‹ç»ä¿¡æ¯: {movie_details['introduction_details'][:50]}..." if len(movie_details['introduction_details']) > 50 else movie_details['introduction_details'])
                    
                    if movie_details.get("trailer_url") and not movie.get("trailer_url"):
                        movie["trailer_url"] = movie_details["trailer_url"]
                        detail_found.append("è·å–åˆ°é¢„å‘Šç‰‡é“¾æ¥")
                
                # å¤„ç†è¯¦ç»†æ”¾æ˜ ä¿¡æ¯
                if detailed_showtimes:
                    # åˆ›å»ºæ˜ å°„æ¥åŒ¹é…è¯¦ç»†æ”¾æ˜ ä¿¡æ¯ä¸åŸå§‹æ”¾æ˜ ä¿¡æ¯
                    detailed_map = {}
                    for ds in detailed_showtimes:
                        key = f"{ds.get('date')}_{ds.get('time')}"
                        detailed_map[key] = ds
                    
                    has_qa_screenings = False
                    has_intro_screenings = False
                    qa_details_found = []
                    intro_details_found = []
                    
                    # éå†ç”µå½±çš„æ‰€æœ‰æ”¾æ˜ æ—¥æœŸ
                    for date_info in movie["show_dates"]:
                        date_str = date_info["date"]
                        
                        # æ›´æ–°æ¯ä¸ªæ”¾æ˜ æ—¶é—´çš„è¯¦ç»†ä¿¡æ¯
                        for time_info in date_info["times"]:
                            time_str = time_info["time"]
                            key = f"{date_str}_{time_str}"
                            
                            # å¦‚æœåœ¨è¯¦ç»†ä¿¡æ¯ä¸­æ‰¾åˆ°åŒ¹é…çš„æ”¾æ˜ 
                            if key in detailed_map:
                                details = detailed_map[key]
                                
                                # æ·»åŠ Q&Aä¿¡æ¯
                                if details.get("has_qa"):
                                    time_info["has_qa"] = True
                                    has_qa_screenings = True
                                    if details.get("special_event"):
                                        time_info["qa_details"] = details["special_event"]
                                        if details["special_event"] not in qa_details_found:
                                            qa_details_found.append(details["special_event"])
                                    if details.get("qa_person"):
                                        time_info["qa_person"] = details["qa_person"]
                                
                                # æ·»åŠ ä»‹ç»ä¿¡æ¯
                                if details.get("has_introduction"):
                                    time_info["has_introduction"] = True
                                    has_intro_screenings = True
                                    if details.get("special_event"):
                                        time_info["introduction_details"] = details["special_event"]
                                        if details["special_event"] not in intro_details_found:
                                            intro_details_found.append(details["special_event"])
                                    if details.get("introduction_person"):
                                        time_info["introduction_person"] = details["introduction_person"]
                                
                                # æ·»åŠ ç‰¹åˆ«æ´»åŠ¨ä¿¡æ¯
                                if details.get("special_event") and not details.get("has_qa") and not details.get("has_introduction"):
                                    time_info["special_event"] = details["special_event"]
                    
                    # æ›´æ–°ç”µå½±çš„Q&Aå’Œä»‹ç»æ ‡å¿—
                    if has_qa_screenings:
                        movie["has_qa"] = True
                        if qa_details_found:
                            movie["qa_details"] = " | ".join(qa_details_found)
                            detail_found.append(f"Q&Aä¿¡æ¯: {movie['qa_details'][:50]}..." if len(movie['qa_details']) > 50 else movie['qa_details'])
                    
                    if has_intro_screenings:
                        movie["has_introduction"] = True
                        if intro_details_found:
                            movie["introduction_details"] = " | ".join(intro_details_found)
                            detail_found.append(f"ä»‹ç»ä¿¡æ¯: {movie['introduction_details'][:50]}..." if len(movie['introduction_details']) > 50 else movie['introduction_details'])
                    
                    detail_found.append(f"è¯¦ç»†æ”¾æ˜ ä¿¡æ¯: {len(detailed_showtimes)} ä¸ªåœºæ¬¡")
                
                if detail_found:
                    logger.info(f"  - å·²è¡¥å……ä¿¡æ¯: {', '.join(detail_found)}")
                else:
                    logger.info("  - æœªæ‰¾åˆ°æ–°å¢ä¿¡æ¯")
                
                # æ˜¾ç¤ºæ‰¾åˆ°çš„ç®€ä»‹
                if movie_details.get("overview_en"):
                    logger.info(f"  - ç”µå½±ç®€ä»‹: {movie_details['overview_en'][:100]}...")
                
                # æ˜¾ç¤ºQ&Aä¿¡æ¯å’ŒIntroductionä¿¡æ¯
                if movie.get("has_qa") and movie.get("qa_details"):
                    logger.info(f"  - Q&Aä¿¡æ¯: {movie['qa_details']}")
                
                if movie.get("has_introduction") and movie.get("introduction_details"):
                    logger.info(f"  - ä»‹ç»ä¿¡æ¯: {movie['introduction_details']}")
                
                # éšæœºå»¶è¿Ÿä»¥é¿å…è¯·æ±‚è¿‡å¿«
                delay = random.uniform(0.5, 1.5)
                logger.info(f"  - {EMOJI['loading']} ç­‰å¾… {delay:.2f} ç§’...")
                await asyncio.sleep(delay)
                
            except Exception as e:
                logger.error(f"è·å– '{movie['title_en']}' è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")
                
            enriched_movies.append(movie)
        
        await page.close()
        
    except Exception as e:
        logger.error(f"{EMOJI['error']} Enrichingç”µå½±è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›åŸå§‹ç”µå½±åˆ—è¡¨
    return movies
    
    return enriched_movies

async def extract_movie_details(page: Page) -> Dict[str, Any]:
    """
    ä»ç”µå½±è¯¦æƒ…é¡µæå–ä¿¡æ¯ï¼Œå…¼å®¹Film Forumæ ¼å¼
    
    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
    
    Returns:
        åŒ…å«ç”µå½±è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
    """
    details = {}
    
    try:
        # è·å–é¡µé¢å†…å®¹
        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–å¯¼æ¼”
        director_tag = soup.select_one('span.director') or soup.find(string=re.compile(r'Director', re.IGNORECASE))
        if director_tag:
            director_text = director_tag.get_text(strip=True) if hasattr(director_tag, 'get_text') else str(director_tag)
            director_match = re.search(r'Director[s]?:\s*([^,]+)', director_text, re.IGNORECASE)
            if director_match:
                details["director"] = director_match.group(1).strip()
        
        # ä»h5æ ‡ç­¾ä¸­æå–å¹´ä»½å’Œæ—¶é•¿ä¿¡æ¯
        # æŸ¥æ‰¾æ ¼å¼å¦‚ "1992 / 115min / DCP" çš„h5æ ‡ç­¾
        h5_tags = soup.find_all('h5')
        for h5 in h5_tags:
            # åŒ¹é…å¹´ä»½/æ—¶é•¿æ ¼å¼
            year_duration_match = re.search(r'(\d{4})\s*/\s*(\d+)min', h5.get_text(strip=True))
            if year_duration_match:
                details["year"] = int(year_duration_match.group(1))
                details["duration"] = f"{year_duration_match.group(2)} min"
                break
        
        # å¦‚æœh5æ ‡ç­¾ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not details.get("year"):
            year_tag = soup.select_one('span.year') or soup.find(string=re.compile(r'\b\d{4}\b'))
            if year_tag:
                year_text = year_tag.get_text(strip=True) if hasattr(year_tag, 'get_text') else str(year_tag)
                year_match = re.search(r'\b(\d{4})\b', year_text)
                if year_match:
                    details["year"] = int(year_match.group(1))
        
        # å¦‚æœæ²¡æ‰¾åˆ°æ—¶é•¿ï¼Œå°è¯•å…¶ä»–æ–¹å¼
        if not details.get("duration"):
            duration_tag = soup.select_one('.duration') or soup.find(string=re.compile(r'\d+\s*min'))
            if duration_tag:
                duration_text = duration_tag.get_text(strip=True) if hasattr(duration_tag, 'get_text') else str(duration_tag)
                duration_match = re.search(r'(\d+)\s*min', duration_text)
                if duration_match:
                    details["duration"] = f"{duration_match.group(1)} min"
        
        # æå–è¯­è¨€
        language_tag = soup.select_one('.language') or soup.find(string=re.compile(r'Language', re.IGNORECASE))
        if language_tag:
            language_text = language_tag.get_text(strip=True) if hasattr(language_tag, 'get_text') else str(language_tag)
            language_match = re.search(r'Language[s]?:\s*([^,]+)', language_text, re.IGNORECASE)
            if language_match:
                details["language"] = language_match.group(1).strip()
        
        # æå–æ¦‚è¿°/ç®€ä»‹
        # 1. é¦–å…ˆå°è¯•ä½¿ç”¨å¸¸è§é€‰æ‹©å™¨
        overview_tag = soup.select_one('.description') or soup.select_one('.overview') or soup.select_one('.synopsis')
        
        # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ‰¾åˆ°æ—¶é•¿åçš„æ®µè½
        if not overview_tag or not overview_tag.get_text(strip=True):
            # å¯»æ‰¾åŒ…å«æ—¶é•¿ä¿¡æ¯çš„h5æ ‡ç­¾
            duration_h5 = soup.find('h5', string=re.compile(r'\d+\s*min'))
            if duration_h5:
                # æ‰¾åˆ°æ—¶é•¿åçš„éç©ºpæ ‡ç­¾
                next_p = duration_h5.find_next('p')
                # å¦‚æœç¬¬ä¸€ä¸ªpæ ‡ç­¾æ˜¯ç©ºçš„ï¼ŒæŸ¥æ‰¾åç»­éç©ºpæ ‡ç­¾
                while next_p and not next_p.get_text(strip=True):
                    next_p = next_p.find_next('p')
                if next_p and next_p.get_text(strip=True):
                    overview_tag = next_p
        
        # 3. å°è¯•åœ¨ç”µå½±ä¿¡æ¯åŒºåŸŸä¸­æ‰¾éç©ºpæ ‡ç­¾
        if not overview_tag or not overview_tag.get_text(strip=True):
            movie_info = soup.select_one('.movie-info')
            if movie_info:
                p_tags = movie_info.select('p')
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéç©ºpæ ‡ç­¾ä½œä¸ºæè¿°
                for p in p_tags:
                    if p.get_text(strip=True):
                        overview_tag = p
                        break
        
        if overview_tag and overview_tag.get_text(strip=True):
            overview_text = overview_tag.get_text(strip=True)
            details["overview_en"] = overview_text
        
        # æ£€æŸ¥ç”µå½±ä¿¡æ¯åŒºåŸŸä¸­çš„æ‰€æœ‰æ®µè½ï¼ŒæŸ¥æ‰¾Q&Aä¿¡æ¯
        details["has_qa"] = False
        movie_info = soup.select_one('.movie-info')
        qa_details = []
        
        if movie_info:
            p_tags = movie_info.select('p')
            for p in p_tags:
                p_text = p.get_text(strip=True)
                if not p_text:
                    continue
                    
                qa_match = re.search(r'Q\s*&\s*A\s+with|Q\s*&\s*A\s+featuring|Q\s*&\s*A\s+by|Q\s*&\s*A\s+moderated\s+by|discussion\s+with', p_text, re.IGNORECASE)
                if qa_match or "Q&A" in p_text:
                    details["has_qa"] = True
                    qa_details.append(p_text)
        
        if details["has_qa"] and qa_details:
            details["qa_details"] = " | ".join(qa_details)
        
        # æ£€æŸ¥ç”µå½±ä¿¡æ¯åŒºåŸŸä¸­çš„æ‰€æœ‰æ®µè½ï¼ŒæŸ¥æ‰¾ä»‹ç»ä¿¡æ¯
        details["has_introduction"] = False
        intro_details = []
        
        if movie_info:
            p_tags = movie_info.select('p')
            for p in p_tags:
                p_text = p.get_text(strip=True)
                if not p_text:
                    continue
                    
                intro_match = re.search(r'introduced\s+by|introduction\s+by|presented\s+by|moderated\s+by', p_text, re.IGNORECASE)
                if intro_match and "Q&A" not in p_text.upper():  # ç¡®ä¿ä¸æ˜¯Q&Açš„ä¸€éƒ¨åˆ†
                    details["has_introduction"] = True
                    intro_details.append(p_text)
        
        if details["has_introduction"] and intro_details:
            details["introduction_details"] = " | ".join(intro_details)
        
        # æå–é¢„å‘Šç‰‡
        trailer_tag = soup.select_one('iframe[src*="youtube"]') or soup.select_one('iframe[src*="vimeo"]') or soup.select_one('lite-youtube')
        if trailer_tag:
            if trailer_tag.name == 'lite-youtube':
                # æå–è§†é¢‘ID
                video_id = trailer_tag.get('videoid', '')
                if video_id:
                    details["trailer_url"] = f"https://www.youtube.com/watch?v={video_id}"
            else:
                details["trailer_url"] = trailer_tag.get('src', '')
        
        # æå–æµ·æŠ¥å›¾ç‰‡
        image_tag = soup.select_one('div.film-poster img') or soup.select_one('.poster img') or soup.select_one('.movie-image img')
        if image_tag:
            image_url = image_tag.get('src', '')
            if image_url:
                if not image_url.startswith('http'):
                    image_url = f"https://metrograph.com{image_url}"
                details["image_url"] = image_url
        
        # æå–ç‰¹åˆ«æ³¨é‡Šï¼ˆå¦‚"NEW 4K RESTORATION"ï¼‰
        note_tag = soup.select_one('.note') or soup.select_one('.special-note')
        if note_tag:
            details["note"] = note_tag.get_text(strip=True)
    
    except Exception as e:
        logger.error(f"è§£æç”µå½±è¯¦æƒ…é¡µæ—¶å‡ºé”™: {str(e)}")
    
    return details

async def extract_showtimes_with_details(page: Page) -> List[Dict]:
    """
    ä»ç”µå½±è¯¦æƒ…é¡µæå–æ¯ä¸ªæ”¾æ˜ åœºæ¬¡çš„è¯¦ç»†ä¿¡æ¯ï¼Œç‰¹åˆ«å…³æ³¨Q&Aå’Œç‰¹åˆ«æ´»åŠ¨
    
    Args:
        page: Playwrighté¡µé¢å¯¹è±¡
    
    Returns:
        åŒ…å«æ¯ä¸ªæ”¾æ˜ åœºæ¬¡è¯¦ç»†ä¿¡æ¯çš„åˆ—è¡¨
    """
    showtimes_details = []
    
    try:
        # è·å–é¡µé¢å†…å®¹
        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰æ”¾æ˜ æ—¥æœŸåŒºå—
        date_blocks = soup.select('.showtimes-date') or soup.select('.showtime-date-block')
        
        for date_block in date_blocks:
            # æå–æ—¥æœŸ
            date_heading = date_block.select_one('h3') or date_block.select_one('.date-heading')
            if not date_heading:
                continue
                
            date_text = date_heading.get_text(strip=True)
            
            # æŸ¥æ‰¾è¯¥æ—¥æœŸä¸‹çš„æ‰€æœ‰æ”¾æ˜ æ—¶é—´
            time_items = date_block.select('.showtime-item') or date_block.select('.time-item')
            
            for time_item in time_items:
                showtime_info = {"date": date_text}
                
                # æå–æ—¶é—´
                time_tag = time_item.select_one('.time') or time_item.select_one('span.hour')
                if time_tag:
                    showtime_info["time"] = time_tag.get_text(strip=True)
                else:
                    # å¦‚æœæ‰¾ä¸åˆ°ä¸“é—¨çš„æ—¶é—´æ ‡ç­¾ï¼Œå°è¯•ä»æ•´ä¸ªitemä¸­æå–
                    time_text = time_item.get_text(strip=True)
                    time_match = re.search(r'\b(\d{1,2}:\d{2}(?:\s*[AP]M)?)\b', time_text)
                    if time_match:
                        showtime_info["time"] = time_match.group(1)
                    else:
                        continue  # å¦‚æœæ‰¾ä¸åˆ°æ—¶é—´ä¿¡æ¯ï¼Œè·³è¿‡æ­¤é¡¹
                
                # æ£€æŸ¥æ˜¯å¦å”®ç½„
                sold_out = False
                sold_out_tag = time_item.select_one('.sold-out') or time_item.select_one('.unavailable')
                if sold_out_tag or 'sold out' in time_item.get_text(strip=True).lower():
                    sold_out = True
                showtime_info["sold_out"] = sold_out
                
                # æå–è´­ç¥¨é“¾æ¥
                ticket_link = None
                link_tag = time_item.select_one('a')
                if link_tag and not sold_out:
                    ticket_link = link_tag.get('href', '')
                    if ticket_link and not ticket_link.startswith('http'):
                        if 'ticketing' in ticket_link.lower():
                            ticket_link = f"https://t.metrograph.com{ticket_link}"
                        else:
                            ticket_link = f"https://metrograph.com{ticket_link}"
                showtime_info["ticket_url"] = ticket_link
                
                # æŸ¥æ‰¾ç‰¹åˆ«æ´»åŠ¨ä¿¡æ¯ï¼Œå¦‚Q&A
                event_info = time_item.select_one('.event-info') or time_item.select_one('.special-info')
                if event_info:
                    event_text = event_info.get_text(strip=True)
                    showtime_info["special_event"] = event_text
                    
                    # åˆ¤æ–­æ˜¯å¦åŒ…å«Q&A
                    qa_match = re.search(r'Q\s*&\s*A|discussion', event_text, re.IGNORECASE)
                    showtime_info["has_qa"] = bool(qa_match)
                    
                    # åˆ¤æ–­æ˜¯å¦åŒ…å«introduction
                    intro_match = re.search(r'intro|presented by', event_text, re.IGNORECASE)
                    showtime_info["has_introduction"] = bool(intro_match)
                else:
                    # å°è¯•ä»æ•´ä¸ªitemæ–‡æœ¬ä¸­æŸ¥æ‰¾ç‰¹åˆ«äº‹ä»¶ä¿¡æ¯
                    item_text = time_item.get_text(strip=True)
                    
                    # ç§»é™¤æ—¶é—´éƒ¨åˆ†ï¼Œåªçœ‹å…¶ä½™æ–‡æœ¬æ˜¯å¦æœ‰ç‰¹åˆ«äº‹ä»¶
                    time_pattern = r'\b\d{1,2}:\d{2}(?:\s*[AP]M)?\b'
                    event_text = re.sub(time_pattern, '', item_text).strip()
                    
                    if event_text and event_text != "Buy Tickets" and event_text != "Sold Out":
                        showtime_info["special_event"] = event_text
                        showtime_info["has_qa"] = 'q&a' in event_text.lower() or 'discussion' in event_text.lower()
                        showtime_info["has_introduction"] = 'intro' in event_text.lower() or 'presented by' in event_text.lower()
                
                showtimes_details.append(showtime_info)
        
        # å¦‚æœç›´æ¥ä»æ”¾æ˜ åŒºå—æ— æ³•æ‰¾åˆ°ä¿¡æ¯ï¼Œå°è¯•ä»é¡µé¢å…¶ä»–ä½ç½®æŸ¥æ‰¾Q&Aå’Œç‰¹åˆ«æ´»åŠ¨ä¿¡æ¯
        if not any(st.get("has_qa") or st.get("has_introduction") for st in showtimes_details):
            # åœ¨æ•´ä¸ªé¡µé¢ä¸­æŸ¥æ‰¾Q&Aæˆ–ä»‹ç»ä¿¡æ¯
            movie_info = soup.select_one('.movie-info') or soup.select_one('.film-info')
            if movie_info:
                info_text = movie_info.get_text(strip=True)
                qa_match = re.search(r'Q\s*&\s*A\s+with\s+([^\.]+)', info_text, re.IGNORECASE)
                intro_match = re.search(r'introduced\s+by\s+([^\.]+)', info_text, re.IGNORECASE)
                
                # å¦‚æœæ‰¾åˆ°ï¼Œå°†ä¿¡æ¯åº”ç”¨åˆ°æ‰€æœ‰åœºæ¬¡
                if qa_match or intro_match:
                    for showtime in showtimes_details:
                        if qa_match:
                            showtime["has_qa"] = True
                            showtime["qa_person"] = qa_match.group(1).strip()
                        if intro_match:
                            showtime["has_introduction"] = True
                            showtime["introduction_person"] = intro_match.group(1).strip()
    
    except Exception as e:
        logger.error(f"æå–æ”¾æ˜ åœºæ¬¡è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
    
    return showtimes_details

async def get_detailed_showtimes(movie_url: str, browser) -> List[Dict]:
    """
    è·å–ç”µå½±è¯¦æƒ…é¡µä¸­çš„æ¯ä¸ªæ”¾æ˜ åœºæ¬¡çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        movie_url: ç”µå½±è¯¦æƒ…é¡µURL
        browser: æµè§ˆå™¨å®ä¾‹
    
    Returns:
        åŒ…å«æ¯ä¸ªæ”¾æ˜ åœºæ¬¡è¯¦ç»†ä¿¡æ¯çš„åˆ—è¡¨
    """
    logger.info(f"è·å–ç”µå½±è¯¦æƒ…é¡µçš„æ”¾æ˜ åœºæ¬¡è¯¦ç»†ä¿¡æ¯: {movie_url}")
    
    page = await browser.new_page()
    detailed_showtimes = []
    
    try:
        await page.goto(movie_url, timeout=30000)
        await page.wait_for_load_state("networkidle")
        
        detailed_showtimes = await extract_showtimes_with_details(page)
        logger.info(f"ä»ç”µå½±è¯¦æƒ…é¡µè·å–äº† {len(detailed_showtimes)} ä¸ªæ”¾æ˜ åœºæ¬¡çš„è¯¦ç»†ä¿¡æ¯")
        
        # æ ¼å¼åŒ–æ—¥æœŸä¸ºYYYY-MM-DDæ ¼å¼
        for showtime in detailed_showtimes:
            if "date" in showtime:
                try:
                    # å°è¯•è§£ææ—¥æœŸæ ¼å¼
                    date_text = showtime["date"]
                    # å¸¸è§æ ¼å¼ï¼š'Friday March 28, 2025' æˆ– 'March 28'
                    for fmt in ["%A %B %d, %Y", "%B %d, %Y", "%B %d"]:
                        try:
                            # å¦‚æœç¼ºå°‘å¹´ä»½ï¼Œæ·»åŠ å½“å‰å¹´ä»½
                            if "%Y" not in fmt:
                                current_year = datetime.now().year
                                parsed_date = datetime.strptime(f"{date_text}, {current_year}", f"{fmt}, %Y")
                            else:
                                parsed_date = datetime.strptime(date_text, fmt)
                            
                            showtime["date"] = parsed_date.strftime("%Y-%m-%d")
                            break
                        except ValueError:
                            continue
                except Exception as e:
                    logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {showtime.get('date')}, é”™è¯¯: {str(e)}")
    
    except Exception as e:
        logger.error(f"è·å–æ”¾æ˜ åœºæ¬¡è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
    
    finally:
        await page.close()
    
    return detailed_showtimes

@log_execution_time
async def main():
    """ä¸»å‡½æ•°"""
    try:
        logger.info(f"{EMOJI['start']} å¼€å§‹çˆ¬å–Metrographç”µå½±ä¿¡æ¯...")
        await scrape_metrograph_all()
    except Exception as e:
        logger.error(f"{EMOJI['error']} çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
# æ·»åŠ ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶
def cleanup_old_logs(log_dir, max_age_days=30):
    """æ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶"""
    try:
        current_time = datetime.now()
        log_path = Path(log_dir)
        
        # åªå¤„ç† .log æ–‡ä»¶
        for log_file in log_path.glob("*.log*"):
            file_age = current_time - datetime.fromtimestamp(log_file.stat().st_mtime)
            if file_age.days > max_age_days:
                logger.info(f"æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶: {log_file}")
                log_file.unlink()
                
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§æ—¥å¿—æ—¶å‡ºé”™: {str(e)}")
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

if __name__ == "__main__":
    # åœ¨å¯åŠ¨æ—¶æ‰“å°ç³»ç»Ÿä¿¡æ¯
    process = psutil.Process(os.getpid())
    logger.info(f"å¼€å§‹ç¨‹åºï¼Œè¿›ç¨‹ID: {os.getpid()}")
    logger.info(f"Pythonç‰ˆæœ¬: {sys.version}")
    logger.info(f"æ“ä½œç³»ç»Ÿ: {sys.platform}")
    logger.info(f"åˆå§‹å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB")
    
    # å¯åŠ¨çˆ¬è™«
    asyncio.run(main())