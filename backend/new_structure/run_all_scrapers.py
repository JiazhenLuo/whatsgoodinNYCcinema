import os
import sys
import time
import subprocess
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(f"logs/run_scrapers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    ]
)
logger = logging.getLogger("scraper_runner")

def run_scraper(script_name):
    """è¿è¡ŒæŒ‡å®šçš„çˆ¬è™«è„šæœ¬"""
    logger.info(f"å¼€å§‹è¿è¡Œçˆ¬è™«: {script_name}")
    start_time = time.time()
    
    try:
        # ä½¿ç”¨subprocessè¿è¡ŒPythonè„šæœ¬
        result = subprocess.run(
            ["python", f"backend/scrapers/{script_name}.py"],
            capture_output=True,
            text=True,
            check=True
        )
        
        # è®°å½•è¾“å‡º
        logger.info(f"{script_name} è¾“å‡º:")
        for line in result.stdout.splitlines():
            logger.info(f"  {line}")
        
        if result.stderr:
            logger.warning(f"{script_name} é”™è¯¯è¾“å‡º:")
            for line in result.stderr.splitlines():
                logger.warning(f"  {line}")
        
        elapsed_time = time.time() - start_time
        logger.info(f"âœ… {script_name} çˆ¬è™«è¿è¡Œå®Œæˆï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’")
        return True
    
    except subprocess.CalledProcessError as e:
        logger.error(f"âŒ {script_name} çˆ¬è™«è¿è¡Œå¤±è´¥: {str(e)}")
        logger.error(f"é”™è¯¯è¾“å‡º: {e.stderr}")
        return False
    
    except Exception as e:
        logger.error(f"âŒ è¿è¡Œ {script_name} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return False

def import_data_to_db():
    """å¯¼å…¥æ‰€æœ‰æ•°æ®åˆ°æ•°æ®åº“"""
    logger.info("å¼€å§‹å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“...")
    
    try:
        # å¯¼å…¥æ•°æ®åº“æ¨¡å—
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        import db
        
        # åˆå§‹åŒ–æ•°æ®åº“
        db.init_db()
        
        # å¯¼å…¥å„ä¸ªå½±é™¢æ•°æ®
        logger.info("å¯¼å…¥ Metrograph æ•°æ®...")
        db.import_metrograph_data()
        
        logger.info("å¯¼å…¥ Film Forum æ•°æ®...")
        db.import_filmforum_data()
        
        logger.info("å¯¼å…¥ IFC æ•°æ®...")
        db.import_ifc_data()
        
        logger.info("âœ… æ‰€æœ‰æ•°æ®å¯¼å…¥å®Œæˆ")
        return True
    
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("å¼€å§‹è¿è¡Œæ‰€æœ‰çˆ¬è™«å’Œå¯¼å…¥æ•°æ®")
    logger.info("=" * 50)
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)
    
    # è¿è¡Œå„ä¸ªçˆ¬è™«
    scrapers = ["metrograph", "filmforum", "ifc"]
    success_count = 0
    
    for scraper in scrapers:
        if run_scraper(scraper):
            success_count += 1
    
    logger.info(f"çˆ¬è™«è¿è¡Œå®Œæˆ: {success_count}/{len(scrapers)} ä¸ªçˆ¬è™«æˆåŠŸ")
    
    # å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªçˆ¬è™«æˆåŠŸï¼Œåˆ™å¯¼å…¥æ•°æ®
    if success_count > 0:
        import_result = import_data_to_db()
        if import_result:
            logger.info("ğŸ‰ æ•´ä¸ªè¿‡ç¨‹å®Œæˆï¼Œçˆ¬è™«æ•°æ®å·²æˆåŠŸå¯¼å…¥åˆ°æ•°æ®åº“")
        else:
            logger.error("âš ï¸ çˆ¬è™«å®Œæˆï¼Œä½†æ•°æ®å¯¼å…¥è¿‡ç¨‹å¤±è´¥")
    else:
        logger.error("âŒ æ‰€æœ‰çˆ¬è™«å‡å¤±è´¥ï¼Œä¸è¿›è¡Œæ•°æ®å¯¼å…¥")
    
    logger.info("=" * 50)

if __name__ == "__main__":
    main() 