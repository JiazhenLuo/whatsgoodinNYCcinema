import logging
import os
import re
import json
import asyncio
import traceback
import sys
from pathlib import Path
from datetime import datetime, timedelta
from logging.handlers import RotatingFileHandler
from typing import Dict, List, Any

from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page

# è®¾ç½®æ—¥å¿—
def setup_logger(log_file=None):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶"""
    logger = logging.getLogger("filmforum_scraper")
    logger.setLevel(logging.INFO)
    logger.handlers = []  # æ¸…é™¤ç°æœ‰handlers

    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¦‚æœæŒ‡å®šäº†æ—¥å¿—æ–‡ä»¶ï¼‰
    if log_file:
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        # ä½¿ç”¨RotatingFileHandleræ›¿ä»£FileHandler
        file_handler = RotatingFileHandler(
            log_file, 
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5,
            encoding='utf-8'
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

# è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f"filmforum_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logger(log_file)

# è®¾ç½®URLå’Œè¾“å‡ºè·¯å¾„
BASE_URL = "https://filmforum.org"
NOW_PLAYING_URL = f"{BASE_URL}/now_playing"
OUTPUT_DIR = "database"
OUTPUT_FILE = "filmforum_movies.json"

# ç‰¹æ®Šå­—ç¬¦è¡¨æƒ…
EMOJI = {
    "start": "âœ¨",
    "calendar": "ğŸ“…",
    "movie": "ğŸ¬",
    "success": "âœ…",
    "error": "âŒ",
    "warning": "âš ï¸",
    "info": "â„¹ï¸",
    "date": "ğŸ“†",
    "time": "â°",
    "loading": "â³"
}

async def scrape_filmforum():
    """çˆ¬å–Film Forumç”µå½±ä¿¡æ¯çš„ä¸»å‡½æ•°"""
    logger.info(f"{EMOJI['start']} å¼€å§‹çˆ¬å–Film Forumç”µå½±ä¿¡æ¯...")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        try:
            page = await browser.new_page()
            
            # è®¿é—®æ”¾æ˜ ä¸­é¡µé¢
            logger.info(f"è®¿é—®Film Forumç”µå½±é¡µé¢: {NOW_PLAYING_URL}")
            await page.goto(NOW_PLAYING_URL, timeout=60000)
            await page.wait_for_load_state("networkidle")
            
            # è§£æHTML
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            
            # æ‰¾åˆ°æ”¾æ˜ æ—¶é—´è¡¨å®¹å™¨
            showtimes_table = soup.find("div", class_="module showtimes-table")
            if not showtimes_table:
                logger.error(f"{EMOJI['error']} æœªæ‰¾åˆ°æ”¾æ˜ æ—¶é—´è¡¨å®¹å™¨")
                return []
            
            # è·å–å½“å‰æ—¥æœŸå’Œæ˜ŸæœŸ
            today = datetime.now()
            weekdays = ["mon", "tue", "wed", "thu", "fri", "sat", "sun"]
            logger.info(f"å½“å‰æ—¥æœŸ: {today.strftime('%Y-%m-%d')}")
            
            # æ‰¾åˆ°å½“å‰æ´»åŠ¨çš„æ ‡ç­¾é¡µ(æ˜ŸæœŸå‡ )
            active_tab = showtimes_table.find("li", class_="ui-tabs-active")
            if not active_tab:
                logger.error(f"{EMOJI['error']} æœªæ‰¾åˆ°æ´»åŠ¨æ ‡ç­¾é¡µ")
                return []
                
            active_weekday = None
            for day_class in active_tab["class"]:
                if day_class.lower() in weekdays:
                    active_weekday = day_class.lower()
                    break
            
            if not active_weekday:
                logger.error(f"{EMOJI['error']} æœªæ‰¾åˆ°æ´»åŠ¨æ˜ŸæœŸ")
                return []
                
            logger.info(f"å½“å‰æ´»åŠ¨æ˜ŸæœŸ: {active_weekday}")
            
            # è®¡ç®—æ—¥æœŸæ˜ å°„
            date_mapping = {}
            active_index = weekdays.index(active_weekday)
            for i, day in enumerate(weekdays):
                # è®¡ç®—ä¸æ´»åŠ¨æ—¥çš„åç§»é‡
                offset = (i - active_index) % 7
                target_date = today + timedelta(days=offset)
                date_mapping[day] = target_date.strftime("%Y-%m-%d")
            
            logger.info(f"æ—¥æœŸæ˜ å°„: {date_mapping}")
            
            # æå–æ‰€æœ‰ç”µå½±
            movie_dict = {}  # ç”¨äºå­˜å‚¨ç”µå½±ä¿¡æ¯ï¼Œä»¥æ ‡é¢˜ä¸ºé”®
            
            # è·å–æ‰€æœ‰æ ‡ç­¾é¡µå†…å®¹
            tabs = showtimes_table.find_all("div", id=lambda x: x and x.startswith("tabs-"))
            logger.info(f"æ‰¾åˆ° {len(tabs)} ä¸ªæ ‡ç­¾é¡µ")
            
            for tab_index, tab in enumerate(tabs):
                tab_id = tab.get("id", "")
                logger.info(f"å¤„ç†æ ‡ç­¾é¡µ {tab_index+1}/{len(tabs)}: {tab_id}")
                
                # æ‰¾åˆ°å¯¹åº”çš„æ˜ŸæœŸå‡ æ ‡ç­¾
                weekday_li = showtimes_table.find("li", attrs={"aria-controls": tab_id})
                if not weekday_li:
                    logger.warning(f"{EMOJI['warning']} æœªæ‰¾åˆ°æ ‡ç­¾é¡µ {tab_id} å¯¹åº”çš„æ˜ŸæœŸæ ‡ç­¾")
                    continue
                    
                weekday = None
                for day_class in weekday_li["class"]:
                    if day_class.lower() in weekdays:
                        weekday = day_class.lower()
                        break
                
                if not weekday:
                    logger.warning(f"{EMOJI['warning']} æœªæ‰¾åˆ°æ ‡ç­¾é¡µ {tab_id} å¯¹åº”çš„æ˜ŸæœŸ")
                    continue
                
                # è·å–æ—¥æœŸ
                date_str = date_mapping.get(weekday, "Unknown")
                logger.info(f"  æ˜ŸæœŸ {weekday} å¯¹åº”æ—¥æœŸ: {date_str}")
                
                # è§£æè¿™ä¸ªæ ‡ç­¾é¡µä¸­çš„æ‰€æœ‰ç”µå½±
                current_series = None
                movie_count = 0
                
                for p in tab.find_all("p"):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç³»åˆ—åç§°
                    series_link = p.find("a", href=lambda x: x and "series" in x)
                    if series_link and not p.find("strong"):
                        current_series = series_link.text.strip()
                        logger.info(f"  æ‰¾åˆ°ç³»åˆ—: {current_series}")
                        continue
                    
                    # è·å–ç”µå½±é“¾æ¥å’Œæ ‡é¢˜
                    movie_link = p.find("strong")
                    if not movie_link:
                        continue
                    
                    movie_a = movie_link.find("a")
                    if not movie_a:
                        continue
                    
                    # è·å–ç”µå½±åŸºæœ¬ä¿¡æ¯
                    movie_url = movie_a.get("href", "")
                    movie_title = movie_a.text.strip()
                    
                    # å¤„ç†å¯¼æ¼”ä¿¡æ¯(å¦‚æœåœ¨æ ‡é¢˜è¡Œä¸­)
                    director = None
                    director_links = p.find_all("a")
                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªä¸æ˜¯ç”µå½±æ ‡é¢˜çš„é“¾æ¥æ˜¯å¦å¯èƒ½æ˜¯å¯¼æ¼”
                    for link in director_links:
                        if link.text.strip() != movie_title and ("'" in link.text or "'" in link.text):
                            director = link.text.strip().replace("'s", "").replace("'s", "")
                            break
                    
                    # è·å–æ”¾æ˜ æ—¶é—´
                    times = []
                    for span in p.find_all("span"):
                        # è·³è¿‡æç¤ºä¿¡æ¯
                        if span.get("class") and "alert" in span.get("class"):
                            continue
                        
                        time_text = span.text.strip()
                        if re.match(r'\d{1,2}:\d{2}', time_text):
                            # æ„å»ºä¸€ä¸ªæ›´å¯é çš„ç¥¨åŠ¡URL
                            safe_title = movie_title.lower().replace(' ', '-').replace("'", "").replace(":", "").replace("?", "")
                            ticket_url = f"https://my.filmforum.org/tickets/{safe_title}"
                            
                            # æ£€æŸ¥æ˜¯å¦å”®ç½„
                            sold_out = "sold out" in span.text.lower()
                            
                            times.append({
                                "time": time_text,
                                "ticket_url": ticket_url,
                                "sold_out": sold_out
                            })
                    
                    if not times:
                        logger.warning(f"  ç”µå½± '{movie_title}' æœªæ‰¾åˆ°æ”¾æ˜ æ—¶é—´")
                        continue
                        
                    logger.info(f"  æ‰¾åˆ°ç”µå½±: {movie_title} ({len(times)} åœºæ”¾æ˜ )")
                    
                    # å°†ç”µå½±æ·»åŠ åˆ°å­—å…¸æˆ–æ›´æ–°ç°æœ‰æ¡ç›®
                    if movie_title in movie_dict:
                        # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯¥æ—¥æœŸ
                        date_exists = False
                        for d in movie_dict[movie_title]["show_dates"]:
                            if d["date"] == date_str:
                                # åˆå¹¶æ”¾æ˜ æ—¶é—´ï¼Œé¿å…é‡å¤
                                existing_times = {t["time"] for t in d["times"]}
                                for new_time in times:
                                    if new_time["time"] not in existing_times:
                                        d["times"].append(new_time)
                                        existing_times.add(new_time["time"])
                                date_exists = True
                                break
                        
                        if not date_exists:
                            movie_dict[movie_title]["show_dates"].append({
                                "date": date_str,
                                "times": times
                            })
                    else:
                        # åˆ›å»ºæ–°ç”µå½±æ¡ç›®
                        movie_dict[movie_title] = {
                            "title_en": movie_title,
                            "detail_url": movie_url if movie_url.startswith("http") else f"{BASE_URL}{movie_url}",
                            "director": director,
                            "series": current_series,
                            "cinema": "Film Forum",
                            "show_dates": [{
                                "date": date_str,
                                "times": times
                            }]
                        }
                        movie_count += 1
                
                logger.info(f"  åœ¨ {date_str} æ‰¾åˆ° {movie_count} éƒ¨ç”µå½±")
            
            # è®¿é—®æ¯éƒ¨ç”µå½±çš„è¯¦æƒ…é¡µè·å–æ›´å¤šä¿¡æ¯
            await enrich_movies_details(browser, movie_dict)
            
            # å°†å­—å…¸å€¼è½¬æ¢ä¸ºåˆ—è¡¨
            movies = list(movie_dict.values())
            logger.info(f"{EMOJI['success']} çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(movies)} éƒ¨ç”µå½±ä¿¡æ¯")
            
            return movies
            
        except Exception as e:
            logger.error(f"{EMOJI['error']} çˆ¬å–Film Forumç”µå½±ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
            logger.error(traceback.format_exc())
            return []
        finally:
            await browser.close()

async def enrich_movies_details(browser, movie_dict):
    """è®¿é—®æ¯éƒ¨ç”µå½±çš„è¯¦æƒ…é¡µï¼Œè¡¥å……è¯¦ç»†ä¿¡æ¯"""
    logger.info(f"{EMOJI['info']} å¼€å§‹è¡¥å……ç”µå½±è¯¦ç»†ä¿¡æ¯...")
    
    page = await browser.new_page()
    
    try:
        movie_count = len(movie_dict)
        for i, (title, movie) in enumerate(movie_dict.items()):
            try:
                logger.info(f"[{i+1}/{movie_count}] è·å–ç”µå½±è¯¦æƒ…: {title}")
                
                # è®¿é—®è¯¦æƒ…é¡µ
                detail_url = movie["detail_url"]
                logger.info(f"  è®¿é—®: {detail_url}")
                
                await page.goto(detail_url, timeout=60000)
                await page.wait_for_load_state("networkidle")
                
                detail_soup = BeautifulSoup(await page.content(), "html.parser")
                
                # è·å–ä¸»æ ‡é¢˜
                main_title = detail_soup.find("h2", class_="main-title")
                if main_title:
                    movie["title_en"] = main_title.text.strip()
                    logger.info(f"  ç¡®è®¤æ ‡é¢˜: {movie['title_en']}")
                
                # æ–¹æ³•1: å°è¯•ä»slideshowä¸­è·å–å›¾ç‰‡
                slideshow = detail_soup.find("ul", class_="slides")
                if slideshow and slideshow.find("li") and slideshow.find("li").find("img"):
                    img_url = slideshow.find("li").find("img").get("src", "")
                    if img_url:
                        movie["image_url"] = img_url if img_url.startswith("http") else f"{BASE_URL}{img_url}"
                        logger.info(f"  æ‰¾åˆ°slideshowå›¾ç‰‡")
                
                # æ–¹æ³•2ï¼šå¦‚æœslideshowä¸­æ²¡æœ‰å›¾ç‰‡ï¼Œå°è¯•ä»hero-imageåŒºåŸŸè·å–
                if not movie.get("image_url"):
                    hero_img = detail_soup.find("div", class_="hero-image")
                    if hero_img and hero_img.find("img"):
                        img_url = hero_img.find("img").get("src", "")
                        if img_url:
                            movie["image_url"] = img_url if img_url.startswith("http") else f"{BASE_URL}{img_url}"
                            logger.info(f"  æ‰¾åˆ°hero-imageå›¾ç‰‡")
                
                # æ–¹æ³•3ï¼šå°è¯•ä»é¡µé¢ä¸Šä»»ä½•ä½ç½®è·å–ç”µå½±ç›¸å…³å›¾ç‰‡
                if not movie.get("image_url"):
                    # æŸ¥æ‰¾ä»»ä½•å¯èƒ½çš„imgæ ‡ç­¾ï¼Œç­›é€‰å‡ºä¸ç”µå½±æ ‡é¢˜ç›¸å…³çš„
                    all_imgs = detail_soup.find_all("img")
                    for img in all_imgs:
                        img_src = img.get("src", "")
                        img_alt = img.get("alt", "")
                        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦ä¸ç”µå½±æ ‡é¢˜ç›¸å…³
                        if (title.lower() in img_src.lower() or 
                            (img_alt and title.lower() in img_alt.lower())):
                            movie["image_url"] = img_src if img_src.startswith("http") else f"{BASE_URL}{img_src}"
                            logger.info(f"  æ‰¾åˆ°ç›¸å…³å›¾ç‰‡")
                            break
                
                # ä»urgentåŒºåŸŸè·å–å¯¼æ¼”ä¿¡æ¯
                urgent_div = detail_soup.find("div", class_="urgent")
                if urgent_div and urgent_div.find("p"):
                    urgent_text = urgent_div.find("p").text.strip()
                    if "DIRECTED BY" in urgent_text:
                        movie["director"] = urgent_text.replace("DIRECTED BY", "").strip()
                        logger.info(f"  å¯¼æ¼”: {movie['director']}")
                    else:
                        movie["note"] = urgent_text
                        logger.info(f"  ç‰¹åˆ«è¯´æ˜: {movie['note']}")
                
                # ä»copyåŒºåŸŸè·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯
                copy_div = detail_soup.find("div", class_="copy")
                if copy_div:
                    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ®µè½ï¼Œå®ƒå¯èƒ½åŒ…å«å¹´ä»½ã€æ—¶é•¿ç­‰ä¿¡æ¯
                    first_p = copy_div.find("p")
                    if first_p:
                        p_text = first_p.text.strip()
                        
                        # æå–å¹´ä»½
                        year_match = re.search(r'\b(19\d{2}|20\d{2})\b', p_text)
                        if year_match:
                            movie["year"] = int(year_match.group(1))
                            logger.info(f"  å¹´ä»½: {movie['year']}")
                        
                        # æå–æ—¶é•¿
                        duration_match = re.search(r'(\d+)\s*min\.', p_text, re.IGNORECASE)
                        if duration_match:
                            movie["duration"] = f"{duration_match.group(1)} min"
                            logger.info(f"  æ—¶é•¿: {movie['duration']}")
                        
                        # æå–è¯­è¨€
                        language_match = re.search(r'In\s+([A-Za-z]+)\s+with', p_text, re.IGNORECASE)
                        if language_match:
                            movie["language"] = language_match.group(1)
                            logger.info(f"  è¯­è¨€: {movie['language']}")
                    
                    # è·å–ç®€ä»‹(é€šå¸¸æ˜¯ç¬¬äºŒä¸ªæ®µè½)
                    p_tags = copy_div.find_all("p")
                    if len(p_tags) > 1:
                        movie["overview_en"] = p_tags[1].text.strip()
                        logger.info(f"  æ‰¾åˆ°ç”µå½±ç®€ä»‹")
                        
                    # æ£€æŸ¥æ˜¯å¦æœ‰Q&Aæˆ–ä»‹ç»åœºæ¬¡
                    movie["has_qa"] = False
                    movie["has_introduction"] = False
                    
                    # æŸ¥æ‰¾é¡µé¢ä¸­æ‰€æœ‰æ®µè½ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«Q&Aæˆ–ä»‹ç»ä¿¡æ¯
                    all_p = detail_soup.find_all("p")
                    for p in all_p:
                        p_text = p.text.lower()
                        if ("q&a" in p_text or "discussion" in p_text or "conversation" in p_text):
                            movie["has_qa"] = True
                            movie["qa_details"] = p.text.strip()
                            logger.info(f"  å‘ç°Q&Aåœºæ¬¡: {movie['qa_details']}")
                        elif ("introduce" in p_text or "introduction" in p_text or "presented by" in p_text):
                            movie["has_introduction"] = True
                            movie["introduction_details"] = p.text.strip()
                            logger.info(f"  å‘ç°ä»‹ç»åœºæ¬¡: {movie['introduction_details']}")
                
                # è·å–é¢„å‘Šç‰‡é“¾æ¥
                trailer = detail_soup.find("div", class_="flex-video")
                if trailer and trailer.find("iframe"):
                    movie["trailer_url"] = trailer.find("iframe").get("src", "")
                    logger.info(f"  æ‰¾åˆ°é¢„å‘Šç‰‡é“¾æ¥")
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                delay = 1 + (i % 2)  # 1-2ç§’ä¹‹é—´
                logger.info(f"  {EMOJI['loading']} ç­‰å¾… {delay} ç§’...")
                await asyncio.sleep(delay)
                
            except Exception as e:
                logger.error(f"  {EMOJI['error']} è·å–ç”µå½± '{title}' è¯¦æƒ…æ—¶å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())
    
    except Exception as e:
        logger.error(f"{EMOJI['error']} è¡¥å……ç”µå½±è¯¦æƒ…æ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
        logger.error(traceback.format_exc())
    
    finally:
        await page.close()

async def main():
    """ä¸»å‡½æ•°"""
    try:
        # çˆ¬å–ç”µå½±ä¿¡æ¯
        movies = await scrape_filmforum()
        
        if not movies:
            logger.error(f"{EMOJI['error']} æœªè·å–åˆ°æœ‰æ•ˆçš„ç”µå½±æ•°æ®")
            return
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        script_dir = os.path.dirname(os.path.abspath(__file__))
        database_dir = os.path.join(os.path.dirname(script_dir), OUTPUT_DIR)
        os.makedirs(database_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰æ•°æ®ï¼Œå¦‚æœæœ‰åˆ™å®Œå…¨æ›¿æ¢
        output_path = os.path.join(database_dir, OUTPUT_FILE)
        
        # ä¿å­˜JSON - ç›´æ¥ä½¿ç”¨æ–°çˆ¬å–çš„æ•°æ®ï¼Œä¸å†åˆå¹¶æ—§æ•°æ®
        # è¿™æ ·å¯ä»¥é¿å…é‡å¤æ•°æ®
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(movies, f, indent=4, ensure_ascii=False)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_movies = len(movies)
        total_dates = set()
        total_showtimes = 0
        
        for movie in movies:
            for date in movie.get("show_dates", []):
                total_dates.add(date["date"])
                total_showtimes += len(date.get("times", []))
        
        logger.info(f"{EMOJI['success']} çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²å­˜å…¥ {os.path.abspath(output_path)}")
        logger.info(f"  - æ€»è®¡: {total_movies} éƒ¨ç”µå½±")
        logger.info(f"  - æ€»æ”¾æ˜ æ—¥æœŸ: {len(total_dates)} ä¸ª")
        logger.info(f"  - æ€»æ”¾æ˜ åœºæ¬¡: {total_showtimes} åœº")
    
    except Exception as e:
        logger.error(f"{EMOJI['error']} å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    asyncio.run(main()) 